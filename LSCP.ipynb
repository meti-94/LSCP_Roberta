{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 C_data/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"C_data/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=19_000, min_frequency=10, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(x) for x in Path(\".\").glob(\"C_data/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir LSCP_tokenizer\n",
    "tokenizer.save_model(\"LSCP_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./LSCP_tokenizer/vocab.json\",\n",
    "    \"./LSCP_tokenizer/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"علی جمعهها خط قرمزشه. سه ساله میخوایم یه جمعه بریم خونش وقت نمیده بهمون\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"علی جمعهها خط قرمزشه. سه ساله میخوایم یه جمعه بریم خونش وقت نمیده بهمون\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=19_000,\n",
    "    max_position_embeddings=258,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./LSCP_tokenizer\", max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset_1 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_1000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_2 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_2000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_3 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_3000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_4 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_4000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_5 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_5000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_6 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_6000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_7 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_7000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_8 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_8000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_9 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_9000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_10 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_10000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_11 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_11000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_12 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_12000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_13 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_13000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_14 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_14000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_15 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_15000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_16 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_16000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_17 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_17000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_18 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_18000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_19 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_19000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_20 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_20000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_21 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/small_file_21000000.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "dataset_eval = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C_data/eval.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./LSCP_small\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_gpu_train_batch_size=30,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_1+dataset_2+dataset_3+dataset_4+dataset_5+dataset_6+dataset_7+dataset_8+dataset_9+dataset_10+dataset_11+dataset_12+dataset_13+dataset_14+dataset_15+dataset_16+dataset_17+dataset_18+dataset_19+dataset_20+dataset_21,\n",
    "    eval_dataset = dataset_eval,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./LSCP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./LSCP/\",\n",
    "    tokenizer=\"./LSCP_tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"La suno <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persian_bert",
   "language": "python",
   "name": "persian_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
